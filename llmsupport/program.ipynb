{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI21',\n",
       " 'AlephAlpha',\n",
       " 'Anthropic',\n",
       " 'Anyscale',\n",
       " 'Aviary',\n",
       " 'AzureOpenAI',\n",
       " 'Banana',\n",
       " 'BaseLLM',\n",
       " 'Beam',\n",
       " 'Bedrock',\n",
       " 'CTransformers',\n",
       " 'CerebriumAI',\n",
       " 'Cohere',\n",
       " 'Databricks',\n",
       " 'DeepInfra',\n",
       " 'Dict',\n",
       " 'FakeListLLM',\n",
       " 'ForefrontAI',\n",
       " 'GPT4All',\n",
       " 'GooglePalm',\n",
       " 'GooseAI',\n",
       " 'HuggingFaceEndpoint',\n",
       " 'HuggingFaceHub',\n",
       " 'HuggingFacePipeline',\n",
       " 'HuggingFaceTextGenInference',\n",
       " 'HumanInputLLM',\n",
       " 'LlamaCpp',\n",
       " 'Modal',\n",
       " 'MosaicML',\n",
       " 'NLPCloud',\n",
       " 'OpenAI',\n",
       " 'OpenAIChat',\n",
       " 'OpenLM',\n",
       " 'Petals',\n",
       " 'PipelineAI',\n",
       " 'PredictionGuard',\n",
       " 'PromptLayerOpenAI',\n",
       " 'PromptLayerOpenAIChat',\n",
       " 'RWKV',\n",
       " 'Replicate',\n",
       " 'SagemakerEndpoint',\n",
       " 'SelfHostedHuggingFaceLLM',\n",
       " 'SelfHostedPipeline',\n",
       " 'StochasticAI',\n",
       " 'Type',\n",
       " 'VertexAI',\n",
       " 'Writer',\n",
       " '__all__',\n",
       " '__annotations__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'ai21',\n",
       " 'aleph_alpha',\n",
       " 'anthropic',\n",
       " 'anyscale',\n",
       " 'aviary',\n",
       " 'bananadev',\n",
       " 'base',\n",
       " 'beam',\n",
       " 'bedrock',\n",
       " 'cerebriumai',\n",
       " 'cohere',\n",
       " 'ctransformers',\n",
       " 'databricks',\n",
       " 'deepinfra',\n",
       " 'fake',\n",
       " 'forefrontai',\n",
       " 'google_palm',\n",
       " 'gooseai',\n",
       " 'gpt4all',\n",
       " 'huggingface_endpoint',\n",
       " 'huggingface_hub',\n",
       " 'huggingface_pipeline',\n",
       " 'huggingface_text_gen_inference',\n",
       " 'human',\n",
       " 'llamacpp',\n",
       " 'loading',\n",
       " 'modal',\n",
       " 'mosaicml',\n",
       " 'nlpcloud',\n",
       " 'openai',\n",
       " 'openlm',\n",
       " 'petals',\n",
       " 'pipelineai',\n",
       " 'predictionguard',\n",
       " 'promptlayer_openai',\n",
       " 'replicate',\n",
       " 'rwkv',\n",
       " 'sagemaker_endpoint',\n",
       " 'self_hosted',\n",
       " 'self_hosted_hugging_face',\n",
       " 'stochasticai',\n",
       " 'type_to_cls_dict',\n",
       " 'utils',\n",
       " 'vertexai',\n",
       " 'writer']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Importing the libraries\n",
    "import langchain.llms as list_llm_support\n",
    "\n",
    "# Step 2: To list all supported LLM models by LangChain\n",
    "dir (list_llm_support)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
